{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## RNN\n",
        "- 시간에 따라서 정보를 계속해서 전달하는 신경망으로 시퀀스 또는 시리즈 데이터를 처리하는데 있어서 유용\n",
        "- 문장, 시계열 데이터, 음성 신호와 같이 일련의 연속적인 데이터에 대해 RNN은 이전 상태의 정보를 활용해서 다음 상태를 예측하는데 효과적.\n",
        "-  \"나는 어제 __ 먹었다\"라는 문장을 완성하는 예제에서 빈 칸을 채우는 데에는 이전의 단어들이 중요. '나는 어제' 다음에 올 단어를 예측하는 것은 전체 문장의 맥락에 기반해서 이루어지는데 이런 맥락을 이해하는 데에 RNN이 유용\n",
        "- 입력 단계에서, RNN은 두 종류의 입력을 받는데 하나는 현재 단계의 입력값 (예를 들어 문장에서의 현재 단어)이고, 또 다른 하나는 이전 단계에서의 상태값. 이 상태값은 과거의 정보를 캡처하는 역할을 하며 RNN은 이 두 가지 입력을 받아서 새로운 상태를 생성하고, 이것은 다음 단계의 입력으로 사용. 이런 과정을 통해 RNN은 시퀀스 데이터에서 과거의 정보를 전달하면서, 전체 시퀀스의 맥락을 파악\n",
        "- RNN은 \"장기 의존성\" 문제에 대처하는 데 어려움이 있으며 시퀀스가 길어질수록, 이전 시점의 정보가 현재 시점에 영향을 미치는 능력이 점점 약해진다.\n",
        "\n",
        "## LSTM\n",
        "- LSTM은 RNN의 기본 구조에 타임스탭을 가로질러 정보를 나르는 데이터 흐름을 추가.\n",
        "- 이동 상태는 입력 연결과 순환 연결(상태)에 연결"
      ],
      "metadata": {
        "id": "FmTXwH2jyOZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \"Transformer\"\n",
        "- 딥 러닝 모델에서 널리 사용되는 아키텍처 중 하나이며 2017년에 \"Attention is All You Need\"라는 논문에서 처음 소개.\n",
        "- 트랜스포머는 RNN과 LSTM이 시퀀스의 연속적인 특성에 의존하는 것과 대조적으로, 전체 시퀀스를 한 번에 처리하고 각 단어가 다른 모든 단어와의 관계를 동시에 고려\n",
        "- Transformer는 텍스트, 이미지 등 다양한 종류의 데이터를 처리할 수 있는 아키텍처로, NLP(자연어 처리) 분야에서 특히 중요한 역할\n",
        "- OpenAI의 GPT-4, Google의 BERT 등 현재 가장 성능이 좋은 모델들도 Transformer 아키텍처를 기반으로 만들어짐\n",
        "- Transformer는 다음과 같은 주요한 요소들로 구성\n",
        "  - Self-attention Mechanism: 셀프 어텐션(self-attention)은 일반적인 어텐션 메커니즘의 한 종류이며 입력 시퀀스의 각 요소가 다른 요소들과 얼마나 관련되어 있는지를 결정하는데 사용되는 메커니즘이며 이를 통해 모델은 각 단어 또는 픽셀이 문맥 내에서 어떻게 상호 작용하는지를 학습.\n",
        "  - Positional Encoding (위치 인코딩): Transformer는 입력 데이터의 순서를 고려하지 않는다는 단점이 있으며 이 문제를 해결하기 위해, 각 입력 요소의 위치 정보를 추가하는 것이 위치 인코딩. 이를 통해 모델은 단어나 이미지 픽셀의 상대적인 또는 절대적인 위치를 이해할 수 있게 함\n",
        "  - Multi-Head Attention (멀티-헤드 어텐션): 이는 Self-attention 메커니즘을 여러 번 반복하여 서로 다른 '관점'에서 데이터를 처리하는 것을 가능하게 하며 각 '헤드'는 데이터의 서로 다른 특성에 집중하게 된다.\n",
        "  - Feed-Forward Neural Networks (전방향 신경망): 각 Self-attention 단계 이후에는 전방향 신경망이 사용. 이 신경망은 각 위치에서 독립적으로 동작하므로 병렬 계산이 가능.\n",
        "  \n",
        " - Residual Connections(잔차 연결)\n",
        "    - 잔차 연결이란 블록(block) 계산을 건너뛰는 경로를 하나 두는 것을 의미\n",
        "    - 입력을 x , 이번 계산 대상 블록을 F 라고 할 때 잔차 연결은 F(x)+x 로 간단히 실현\n",
        "    - 잔차 연결을 두지 않았을 때는 f1 , f2 , f3 을 연속으로 수행하는 경로 한 가지만 존재하였으나, 잔차 연결을 블록마다 설정해둠으로써 모두 8가지의 새로운 경로가 생성. 다시 말해 모델이 다양한 관점에서 블록 계산을 수행\n",
        "    - 잔차 연결은 모델 중간에 블록을 건너뛰는 경로를 설정함으로써 학습을 용이하게 하는 효과가 있음\n",
        "\n",
        " - Layer Normalization(레이어 정규화)\n",
        "    - 미니 배치의 인스턴스( x )별로 평균을 빼주고 표준편차로 나눠줘 정규화(normalization)을 수행하는 기법\n",
        "    - 레이어 정규화를 수행하면 학습이 안정되고 그 속도가 빨라지는 등의 효과\n",
        "\n",
        "이러한 구성 요소들을 통해 Transformer는 문맥적인 정보를 보다 효과적으로 캡처하며, 이는 기존의 RNN이나 CNN 기반 모델들보다 더 좋은 성능을 낼 수 있도록 한다.\n"
      ],
      "metadata": {
        "id": "VFQfTD7rLfDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 멀티헤드 어텐션 vs 마스크드 멀티헤드 어텐션\n",
        "\n",
        "- 멀티헤드 어텐션: 이 기법에서는 입력 시퀀스의 모든 위치에서 모든 위치로의 어텐션을 계산하며 각 단어는 문장 내의 모든 다른 단어와의 관계를 고려합니다. 이는 트랜스포머의 인코더에서 주로 사용되는 방식.\n",
        "\n",
        "- 마스크드 멀티헤드 어텐션: 이 기법에서는 어텐션 계산이 순차적으로 수행되며 특정 위치의 단어는 해당 위치 이후의 단어에 대한 정보를 '참조'하지 못한다. 이를 위해 '마스킹'이라는 기법을 사용하여 미래의 정보에 대한 어텐션 가중치를 0으로 만든다. 이 방법은 트랜스포머의 디코더에서 사용되며, 예측 과정에서 정보가 순차적으로 생성되어야 하는 것을 보장. 이는 모델이 예측해야 하는 토큰을 '보지 못하게' 함으로써, 미래의 정보를 이용하지 못하게 만든다.\n",
        "\n",
        "- 이 두 기법의 차이점은 '마스킹'에 있으며, 이는 모델이 정보를 처리하는 순서와 방식에 큰 영향을 미친다."
      ],
      "metadata": {
        "id": "lyJILi-ULfFr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gyDBrmxJLfID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cfcPfdtQLfKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GYe9v3XmLfMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sGYQl539LfOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ArEB_JfELfQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6RED6n1QLfTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s8KivbNkLfVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JRunFwJBLfX0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
