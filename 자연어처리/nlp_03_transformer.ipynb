{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## RNN\n",
        "- 시간에 따라서 정보를 계속해서 전달하는 신경망으로 시퀀스 또는 시리즈 데이터를 처리하는데 있어서 유용\n",
        "- 문장, 시계열 데이터, 음성 신호와 같이 일련의 연속적인 데이터에 대해 RNN은 이전 상태의 정보를 활용해서 다음 상태를 예측하는데 효과적.\n",
        "-  \"나는 어제 __ 먹었다\"라는 문장을 완성하는 예제에서 빈 칸을 채우는 데에는 이전의 단어들이 중요. '나는 어제' 다음에 올 단어를 예측하는 것은 전체 문장의 맥락에 기반해서 이루어지는데 이런 맥락을 이해하는 데에 RNN이 유용\n",
        "- 입력 단계에서, RNN은 두 종류의 입력을 받는데 하나는 현재 단계의 입력값 (예를 들어 문장에서의 현재 단어)이고, 또 다른 하나는 이전 단계에서의 상태값. 이 상태값은 과거의 정보를 캡처하는 역할을 하며 RNN은 이 두 가지 입력을 받아서 새로운 상태를 생성하고, 이것은 다음 단계의 입력으로 사용. 이런 과정을 통해 RNN은 시퀀스 데이터에서 과거의 정보를 전달하면서, 전체 시퀀스의 맥락을 파악\n",
        "- RNN은 \"장기 의존성\" 문제에 대처하는 데 어려움이 있으며 시퀀스가 길어질수록, 이전 시점의 정보가 현재 시점에 영향을 미치는 능력이 점점 약해진다.\n",
        "\n",
        "## LSTM\n",
        "- LSTM은 RNN의 기본 구조에 타임스탭을 가로질러 정보를 나르는 데이터 흐름을 추가.\n",
        "- 이동 상태는 입력 연결과 순환 연결(상태)에 연결"
      ],
      "metadata": {
        "id": "FmTXwH2jyOZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \"Transformer\"\n",
        "- 딥 러닝 모델에서 널리 사용되는 아키텍처 중 하나이며 2017년에 \"Attention is All You Need\"라는 논문에서 처음 소개.\n",
        "- 트랜스포머는 RNN과 LSTM이 시퀀스의 연속적인 특성에 의존하는 것과 대조적으로, 전체 시퀀스를 한 번에 처리하고 각 단어가 다른 모든 단어와의 관계를 동시에 고려\n",
        "- Transformer는 텍스트, 이미지 등 다양한 종류의 데이터를 처리할 수 있는 아키텍처로, NLP(자연어 처리) 분야에서 특히 중요한 역할\n",
        "- OpenAI의 GPT-4, Google의 BERT 등 현재 가장 성능이 좋은 모델들도 Transformer 아키텍처를 기반으로 만들어짐\n",
        "- Transformer는 다음과 같은 주요한 요소들로 구성\n",
        "  - Self-attention Mechanism: 셀프 어텐션(self-attention)은 일반적인 어텐션 메커니즘의 한 종류이며 입력 시퀀스의 각 요소가 다른 요소들과 얼마나 관련되어 있는지를 결정하는데 사용되는 메커니즘이며 이를 통해 모델은 각 단어 또는 픽셀이 문맥 내에서 어떻게 상호 작용하는지를 학습.\n",
        "  - Positional Encoding (위치 인코딩): Transformer는 입력 데이터의 순서를 고려하지 않는다는 단점이 있으며 이 문제를 해결하기 위해, 각 입력 요소의 위치 정보를 추가하는 것이 위치 인코딩. 이를 통해 모델은 단어나 이미지 픽셀의 상대적인 또는 절대적인 위치를 이해할 수 있게 함\n",
        "  - Multi-Head Attention (멀티-헤드 어텐션): 이는 Self-attention 메커니즘을 여러 번 반복하여 서로 다른 '관점'에서 데이터를 처리하는 것을 가능하게 하며 각 '헤드'는 데이터의 서로 다른 특성에 집중하게 된다.\n",
        "  - Feed-Forward Neural Networks (전방향 신경망): 각 Self-attention 단계 이후에는 전방향 신경망이 사용. 이 신경망은 각 위치에서 독립적으로 동작하므로 병렬 계산이 가능.\n",
        "  \n",
        " - Residual Connections(잔차 연결)\n",
        "    - 잔차 연결이란 블록(block) 계산을 건너뛰는 경로를 하나 두는 것을 의미\n",
        "    - 입력을 x , 이번 계산 대상 블록을 F 라고 할 때 잔차 연결은 F(x)+x 로 간단히 실현\n",
        "    - 잔차 연결을 두지 않았을 때는 f1 , f2 , f3 을 연속으로 수행하는 경로 한 가지만 존재하였으나, 잔차 연결을 블록마다 설정해둠으로써 모두 8가지의 새로운 경로가 생성. 다시 말해 모델이 다양한 관점에서 블록 계산을 수행\n",
        "    - 잔차 연결은 모델 중간에 블록을 건너뛰는 경로를 설정함으로써 학습을 용이하게 하는 효과가 있음\n",
        "\n",
        " - Layer Normalization(레이어 정규화)\n",
        "    - 미니 배치의 인스턴스( x )별로 평균을 빼주고 표준편차로 나눠줘 정규화(normalization)을 수행하는 기법\n",
        "    - 레이어 정규화를 수행하면 학습이 안정되고 그 속도가 빨라지는 등의 효과\n",
        "\n",
        "이러한 구성 요소들을 통해 Transformer는 문맥적인 정보를 보다 효과적으로 캡처하며, 이는 기존의 RNN이나 CNN 기반 모델들보다 더 좋은 성능을 낼 수 있도록 한다.\n"
      ],
      "metadata": {
        "id": "VFQfTD7rLfDE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 멀티헤드 어텐션 vs 마스크드 멀티헤드 어텐션\n",
        "\n",
        "- 멀티헤드 어텐션: 이 기법에서는 입력 시퀀스의 모든 위치에서 모든 위치로의 어텐션을 계산하며 각 단어는 문장 내의 모든 다른 단어와의 관계를 고려합니다. 이는 트랜스포머의 인코더에서 주로 사용되는 방식.\n",
        "\n",
        "- 마스크드 멀티헤드 어텐션: 이 기법에서는 어텐션 계산이 순차적으로 수행되며 특정 위치의 단어는 해당 위치 이후의 단어에 대한 정보를 '참조'하지 못한다. 이를 위해 '마스킹'이라는 기법을 사용하여 미래의 정보에 대한 어텐션 가중치를 0으로 만든다. 이 방법은 트랜스포머의 디코더에서 사용되며, 예측 과정에서 정보가 순차적으로 생성되어야 하는 것을 보장. 이는 모델이 예측해야 하는 토큰을 '보지 못하게' 함으로써, 미래의 정보를 이용하지 못하게 만든다.\n",
        "\n",
        "- 이 두 기법의 차이점은 '마스킹'에 있으며, 이는 모델이 정보를 처리하는 순서와 방식에 큰 영향을 미친다."
      ],
      "metadata": {
        "id": "lyJILi-ULfFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT(Bidirectional Encoder Representations from Transformers)\n",
        "- 트랜스포머 아키텍처의 인코더만을 사용하는데, 그 이유는 BERT의 주된 목표와 학습 방식 때문이다.\n",
        "-  BERT는 문장의 일부 단어가 마스킹(가려짐)되어 있는 상황에서 마스킹된 단어를 예측하려고 하며 이 과정에서 BERT는 양방향의 문맥 정보를 모두 활용하여 단어의 의미를 이해하게 된다.\n",
        "- 트랜스포머의 인코더는 입력 시퀀스 내의 모든 위치에서 모든 위치로의 어텐션을 계산하므로, 각 단어에 대한 양방향 문맥을 자연스럽게 학습하므로 이러한 이유로 BERT는 트랜스포머의 인코더만을 사용\n",
        "- 디코더는 대부분의 경우에 순차적인 정보 처리를 요구하는데, 이는 BERT의 양방향 문맥 이해 목표와는 부합하지 않기 때문"
      ],
      "metadata": {
        "id": "gyDBrmxJLfID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT (Generative Pretrained Transformer)\n",
        "- 트랜스포머 아키텍처의 디코더만 사용하는데, 그 이유는 GPT의 목표와 학습 방식에 기인.\n",
        "- GPT는 주어진 문맥에 따라 텍스트를 생성하는 것을 목표로 하며 주어진 입력에 대해 가장 가능성 있는 다음 토큰(일반적으로 단어 또는 단어의 일부, s토큰)을 예측하는 문제로 볼 수 있다.\n",
        "- 입력 텍스트를 디코더에 직접 제공하고, 디코더는 각 단계에서 다음 토큰을 예측하게 되어 GPT는 주어진 문맥을 기반으로 텍스트를 생성하는 데 필요한 모델을 학습"
      ],
      "metadata": {
        "id": "cfcPfdtQLfKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 셀프 어텐션(self-attention)\n",
        "- 문장이나 시퀀스 안에서 각각의 단어나 항목들이 서로 어떻게 연결되는지를 파악하는 메커니즘. 딥 러닝 모델이 단어의 문맥을 파악하게 해주는 것\n",
        "- 셀프 어텐션은 어텐션 메커니즘의 한 종류로, 주어진 문장이나 시퀀스 안의 모든 단어 간의 관계를 파악\n",
        "\n",
        "- 셀프 어텐션은 쿼리(query), 키(key), 밸류(value) 3개 요소 사이의 문맥적 관계성을 추출하는 과정\n",
        " - Query (질의): 현재에 집중하고 있는 특정 단어나 토큰을 대표. 질의는 이 토큰이 다른 토큰과 어떤 관계를 가지는지를 결정하는 데 사용.\n",
        " - Key (키): 입력 시퀀스의 각 단어나 토큰을 대표. 키는 질의와 비교되어, 질의가 특정 토큰에 얼마나 '집중'해야 하는지를 결정하는 데 사용.\n",
        " - Value (값): 입력 시퀀스의 각 단어나 토큰을 대표하며, 가중치를 적용하여 최종 출력을 생성하는 데 사용. 이 가중치는 질의와 키의 상호작용을 통해 계산."
      ],
      "metadata": {
        "id": "GYe9v3XmLfMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.tensor([\n",
        "    [1.0,0.0,1.0,0.0],\n",
        "    [0.0,2.0,0.0,2.0],\n",
        "    [1.0,1.0,1.0,1.0],\n",
        "])\n",
        "w_query = torch.tensor([\n",
        "    [1.0,0.0,1.0],\n",
        "    [1.0,0.0,0.0],\n",
        "    [0.0,0.0,1.0],\n",
        "    [0.0,1.0,1.0],\n",
        "])\n",
        "\n",
        "w_key =torch.tensor([\n",
        "    [0.0,0.0,1.0],\n",
        "    [1.0,1.0,0.0],\n",
        "    [0.0,1.0,0.0],\n",
        "    [1.0,1.0,0.0],\n",
        "])\n",
        "w_value = torch.tensor([\n",
        "    [0.0,2.0,0.0],\n",
        "    [0.0,3.0,0.0],\n",
        "    [1.0,0.0,3.0],\n",
        "    [1.0,1.0,0.0],\n",
        "])"
      ],
      "metadata": {
        "id": "sGYQl539LfOs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 쿼리, 키, 벨류 만들기\n",
        "\n",
        "keys = torch.matmul(x,w_key)\n",
        "querys = torch.matmul(x,w_query)\n",
        "values = torch.matmul(x,w_value)"
      ],
      "metadata": {
        "id": "yyT73_R7Vicj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = torch.matmul(querys, keys.T)"
      ],
      "metadata": {
        "id": "ArEB_JfELfQ7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RED6n1QLfTF",
        "outputId": "26545034-e69a-4aa7-a8b8-4dc97a61c433"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.,  4.,  4.],\n",
              "        [ 4., 16., 12.],\n",
              "        [ 4., 12., 10.]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s8KivbNkLfVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JRunFwJBLfX0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}