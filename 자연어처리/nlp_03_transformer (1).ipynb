{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNRRhHQuWoHpb8u/LoBBMTw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## RNN\n","- 시간에 따라서 정보를 계속해서 전달하는 신경망으로 시퀀스 또는 시리즈 데이터를 처리하는데 있어서 유용\n","- 문장, 시계열 데이터, 음성 신호와 같이 일련의 연속적인 데이터에 대해 RNN은 이전 상태의 정보를 활용해서 다음 상태를 예측하는데 효과적.\n","-  \"나는 어제 __ 먹었다\"라는 문장을 완성하는 예제에서 빈 칸을 채우는 데에는 이전의 단어들이 중요. '나는 어제' 다음에 올 단어를 예측하는 것은 전체 문장의 맥락에 기반해서 이루어지는데 이런 맥락을 이해하는 데에 RNN이 유용\n","- 입력 단계에서, RNN은 두 종류의 입력을 받는데 하나는 현재 단계의 입력값 (예를 들어 문장에서의 현재 단어)이고, 또 다른 하나는 이전 단계에서의 상태값. 이 상태값은 과거의 정보를 캡처하는 역할을 하며 RNN은 이 두 가지 입력을 받아서 새로운 상태를 생성하고, 이것은 다음 단계의 입력으로 사용. 이런 과정을 통해 RNN은 시퀀스 데이터에서 과거의 정보를 전달하면서, 전체 시퀀스의 맥락을 파악\n","- RNN은 \"장기 의존성\" 문제에 대처하는 데 어려움이 있으며 시퀀스가 길어질수록, 이전 시점의 정보가 현재 시점에 영향을 미치는 능력이 점점 약해진다.\n","\n","## LSTM\n","- LSTM은 RNN의 기본 구조에 타임스탭을 가로질러 정보를 나르는 데이터 흐름을 추가.\n","- 이동 상태는 입력 연결과 순환 연결(상태)에 연결"],"metadata":{"id":"FmTXwH2jyOZs"}},{"cell_type":"markdown","source":["## \"Transformer\"\n","- 딥 러닝 모델에서 널리 사용되는 아키텍처 중 하나이며 2017년에 \"Attention is All You Need\"라는 논문에서 처음 소개.\n","- 트랜스포머는 RNN과 LSTM이 시퀀스의 연속적인 특성에 의존하는 것과 대조적으로, 전체 시퀀스를 한 번에 처리하고 각 단어가 다른 모든 단어와의 관계를 동시에 고려\n","- Transformer는 텍스트, 이미지 등 다양한 종류의 데이터를 처리할 수 있는 아키텍처로, NLP(자연어 처리) 분야에서 특히 중요한 역할\n","- OpenAI의 GPT-4, Google의 BERT 등 현재 가장 성능이 좋은 모델들도 Transformer 아키텍처를 기반으로 만들어짐\n","- Transformer는 다음과 같은 주요한 요소들로 구성\n","  - Self-attention Mechanism: 셀프 어텐션(self-attention)은 일반적인 어텐션 메커니즘의 한 종류이며 입력 시퀀스의 각 요소가 다른 요소들과 얼마나 관련되어 있는지를 결정하는데 사용되는 메커니즘이며 이를 통해 모델은 각 단어 또는 픽셀이 문맥 내에서 어떻게 상호 작용하는지를 학습.\n","  - Positional Encoding (위치 인코딩): Transformer는 입력 데이터의 순서를 고려하지 않는다는 단점이 있으며 이 문제를 해결하기 위해, 각 입력 요소의 위치 정보를 추가하는 것이 위치 인코딩. 이를 통해 모델은 단어나 이미지 픽셀의 상대적인 또는 절대적인 위치를 이해할 수 있게 함\n","  - Multi-Head Attention (멀티-헤드 어텐션): 이는 Self-attention 메커니즘을 여러 번 반복하여 서로 다른 '관점'에서 데이터를 처리하는 것을 가능하게 하며 각 '헤드'는 데이터의 서로 다른 특성에 집중하게 된다.\n","  - Feed-Forward Neural Networks (전방향 신경망): 각 Self-attention 단계 이후에는 전방향 신경망이 사용. 이 신경망은 각 위치에서 독립적으로 동작하므로 병렬 계산이 가능.\n","  \n"," - Residual Connections(잔차 연결)\n","    - 잔차 연결이란 블록(block) 계산을 건너뛰는 경로를 하나 두는 것을 의미\n","    - 입력을 x , 이번 계산 대상 블록을 F 라고 할 때 잔차 연결은 F(x)+x 로 간단히 실현\n","    - 잔차 연결을 두지 않았을 때는 f1 , f2 , f3 을 연속으로 수행하는 경로 한 가지만 존재하였으나, 잔차 연결을 블록마다 설정해둠으로써 모두 8가지의 새로운 경로가 생성. 다시 말해 모델이 다양한 관점에서 블록 계산을 수행\n","    - 잔차 연결은 모델 중간에 블록을 건너뛰는 경로를 설정함으로써 학습을 용이하게 하는 효과가 있음\n","\n"," - Layer Normalization(레이어 정규화)\n","    - 미니 배치의 인스턴스( x )별로 평균을 빼주고 표준편차로 나눠줘 정규화(normalization)을 수행하는 기법\n","    - 레이어 정규화를 수행하면 학습이 안정되고 그 속도가 빨라지는 등의 효과\n","\n","이러한 구성 요소들을 통해 Transformer는 문맥적인 정보를 보다 효과적으로 캡처하며, 이는 기존의 RNN이나 CNN 기반 모델들보다 더 좋은 성능을 낼 수 있도록 한다.\n"],"metadata":{"id":"VFQfTD7rLfDE"}},{"cell_type":"markdown","source":["#### 멀티헤드 어텐션 vs 마스크드 멀티헤드 어텐션\n","\n","- 멀티헤드 어텐션: 이 기법에서는 입력 시퀀스의 모든 위치에서 모든 위치로의 어텐션을 계산하며 각 단어는 문장 내의 모든 다른 단어와의 관계를 고려합니다. 이는 트랜스포머의 인코더에서 주로 사용되는 방식.\n","\n","- 마스크드 멀티헤드 어텐션: 이 기법에서는 어텐션 계산이 순차적으로 수행되며 특정 위치의 단어는 해당 위치 이후의 단어에 대한 정보를 '참조'하지 못한다. 이를 위해 '마스킹'이라는 기법을 사용하여 미래의 정보에 대한 어텐션 가중치를 0으로 만든다. 이 방법은 트랜스포머의 디코더에서 사용되며, 예측 과정에서 정보가 순차적으로 생성되어야 하는 것을 보장. 이는 모델이 예측해야 하는 토큰을 '보지 못하게' 함으로써, 미래의 정보를 이용하지 못하게 만든다.\n","\n","- 이 두 기법의 차이점은 '마스킹'에 있으며, 이는 모델이 정보를 처리하는 순서와 방식에 큰 영향을 미친다."],"metadata":{"id":"lyJILi-ULfFr"}},{"cell_type":"markdown","source":["## BERT(Bidirectional Encoder Representations from Transformers)\n","- 트랜스포머 아키텍처의 인코더만을 사용하는데, 그 이유는 BERT의 주된 목표와 학습 방식 때문이다.\n","-  BERT는 문장의 일부 단어가 마스킹(가려짐)되어 있는 상황에서 마스킹된 단어를 예측하려고 하며 이 과정에서 BERT는 양방향의 문맥 정보를 모두 활용하여 단어의 의미를 이해하게 된다.\n","- 트랜스포머의 인코더는 입력 시퀀스 내의 모든 위치에서 모든 위치로의 어텐션을 계산하므로, 각 단어에 대한 양방향 문맥을 자연스럽게 학습하므로 이러한 이유로 BERT는 트랜스포머의 인코더만을 사용\n","- 디코더는 대부분의 경우에 순차적인 정보 처리를 요구하는데, 이는 BERT의 양방향 문맥 이해 목표와는 부합하지 않기 때문"],"metadata":{"id":"gyDBrmxJLfID"}},{"cell_type":"markdown","source":["## GPT (Generative Pretrained Transformer)\n","- 트랜스포머 아키텍처의 디코더만 사용하는데, 그 이유는 GPT의 목표와 학습 방식에 기인.\n","- GPT는 주어진 문맥에 따라 텍스트를 생성하는 것을 목표로 하며 주어진 입력에 대해 가장 가능성 있는 다음 토큰(일반적으로 단어 또는 단어의 일부, s토큰)을 예측하는 문제로 볼 수 있다.\n","- 입력 텍스트를 디코더에 직접 제공하고, 디코더는 각 단계에서 다음 토큰을 예측하게 되어 GPT는 주어진 문맥을 기반으로 텍스트를 생성하는 데 필요한 모델을 학습"],"metadata":{"id":"cfcPfdtQLfKT"}},{"cell_type":"markdown","source":["## 셀프 어텐션(self-attention)\n","- 문장이나 시퀀스 안에서 각각의 단어나 항목들이 서로 어떻게 연결되는지를 파악하는 메커니즘. 딥 러닝 모델이 단어의 문맥을 파악하게 해주는 것\n","- 셀프 어텐션은 어텐션 메커니즘의 한 종류로, 주어진 문장이나 시퀀스 안의 모든 단어 간의 관계를 파악\n","\n","- 셀프 어텐션은 쿼리(query), 키(key), 밸류(value) 3개 요소 사이의 문맥적 관계성을 추출하는 과정\n"," - Query (질의): 현재에 집중하고 있는 특정 단어나 토큰을 대표. 질의는 이 토큰이 다른 토큰과 어떤 관계를 가지는지를 결정하는 데 사용.\n"," - Key (키): 입력 시퀀스의 각 단어나 토큰을 대표. 키는 질의와 비교되어, 질의가 특정 토큰에 얼마나 '집중'해야 하는지를 결정하는 데 사용.\n"," - Value (값): 입력 시퀀스의 각 단어나 토큰을 대표하며, 가중치를 적용하여 최종 출력을 생성하는 데 사용. 이 가중치는 질의와 키의 상호작용을 통해 계산."],"metadata":{"id":"GYe9v3XmLfMr"}},{"cell_type":"code","source":["import torch\n","x = torch.tensor([\n","    [1.0,0.0,1.0,0.0],\n","    [0.0,2.0,0.0,2.0],\n","    [1.0,1.0,1.0,1.0],\n","])\n","w_query = torch.tensor([\n","    [1.0,0.0,1.0],\n","    [1.0,0.0,0.0],\n","    [0.0,0.0,1.0],\n","    [0.0,1.0,1.0],\n","])\n","\n","w_key =torch.tensor([\n","    [0.0,0.0,1.0],\n","    [1.0,1.0,0.0],\n","    [0.0,1.0,0.0],\n","    [1.0,1.0,0.0],\n","])\n","w_value = torch.tensor([\n","    [0.0,2.0,0.0],\n","    [0.0,3.0,0.0],\n","    [1.0,0.0,3.0],\n","    [1.0,1.0,0.0],\n","])"],"metadata":{"id":"sGYQl539LfOs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 쿼리, 키, 벨류 만들기\n","\n","keys = torch.matmul(x,w_key)\n","querys = torch.matmul(x,w_query)\n","values = torch.matmul(x,w_value)"],"metadata":{"id":"yyT73_R7Vicj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attn_scores = torch.matmul(querys, keys.T)"],"metadata":{"id":"ArEB_JfELfQ7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attn_scores"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6RED6n1QLfTF","executionInfo":{"status":"ok","timestamp":1689908550157,"user_tz":-540,"elapsed":12,"user":{"displayName":"일땽","userId":"01657347254653910294"}},"outputId":"26545034-e69a-4aa7-a8b8-4dc97a61c433"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 2.,  4.,  4.],\n","        [ 4., 16., 12.],\n","        [ 4., 12., 10.]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":[],"metadata":{"id":"s8KivbNkLfVU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 사례\n","\"나는 김치찌개를 만들어 먹었다.\"라는 문장이 있을 때, 우리가 \"만들어\"라는 단어에 주목하려 한다고 가정. 여기서 \"만들어\"는 쿼리(Query). 이제 문장의 모든 단어들이 키(Key)와 값(Value)를 갖는다.\n","\n","\"나는\": 키(Key)로서 \"나는\"이라는 주체를 나타내고, 값(Value)으로서는 \"나는\"라는 개체에 대한 정보를 갖는다.\n","\n","\"김치찌개를\": 키(Key)로서 \"김치찌개를\"이라는 대상을 나타내고, 값(Value)으로서는 \"김치찌개\"라는 대상에 대한 정보를 갖는다.\n","\n","\"만들어\": 키(Key)로서 \"만들어\"라는 행동을 나타내고, 값(Value)으로서는 \"만들어\"라는 행동에 대한 정보를 갖는다.\n","\n","\"먹었다.\": 키(Key)로서 \"먹었다.\"라는 행동을 나타내고, 값(Value)으로서는 \"먹었다\"라는 행동에 대한 정보를 갖는다.\n","\n","- 입력 준비: 문장의 각 단어에 대해 세 가지 다른 가중치 행렬을 곱하여 쿼리(Query), 키(Key), 값(Value) 벡터를 생성합니다. 이 가중치 행렬들은 모델의 학습 과정에서 최적화됩니다.\n","\n","- 스코어 계산: \"만들어\"라는 쿼리 벡터와 각 키 벡터의 내적을 계산하여 스코어를 계산합니다. 이 스코어는 \"만들어\"라는 단어가 각 단어(\"나는\", \"김치찌개를\", \"만들어\", \"먹었다.\")와 얼마나 관련이 있는지를 나타냅니다.\n","\n","- 스코어 정규화: 계산된 스코어를 softmax 함수를 통해 정규화하여 확률분포를 만듭니다. 이 확률분포는 \"만들어\"라는 단어가 각 단어에 얼마나 가중치를 둘 것인지를 나타냅니다.\n","\n","- 가중합 계산: 정규화된 스코어를 각 값 벡터에 곱한 후, 모두 합하여 새로운 벡터를 생성합니다. 예를 들어, \"김치찌개를\"에 대한 스코어가 높다면, \"김치찌개\"에 대한 값 벡터가 \"만들어\"라는 단어의 새로운 표현에 더 크게 기여하게 됩니다.\n","\n","- 출력 생성: 모든 입력 원소에 대해 위의 과정을 반복하고, 그 결과를 합쳐서 셀프 어텐션 레이어의 최종 출력을 생성합니다. 이렇게 생성된 출력은 다음 레이어로 전달됩니다."],"metadata":{"id":"JRunFwJBLfX0"}},{"cell_type":"code","source":["# 모델이 학습 데이터를 통해 학습하면서, 이 가중치 행렬들은 경사하강법과 같은 최적화 알고리즘에 의해 점자 업데이트 되면서 조정\n","\n","import torch\n","x = torch.tensor([\n","    [1.0,0.0,1.0,0.0],\n","    [0.0,2.0,0.0,2.0],\n","    [1.0,1.0,1.0,1.0],\n","])\n","w_query = torch.tensor([\n","    [1.0,0.0,1.0],\n","    [1.0,0.0,0.0],\n","    [0.0,0.0,1.0],\n","    [0.0,1.0,1.0],\n","])\n","\n","w_key =torch.tensor([\n","    [0.0,0.0,1.0],\n","    [1.0,1.0,0.0],\n","    [0.0,1.0,0.0],\n","    [1.0,1.0,0.0],\n","])\n","w_value = torch.tensor([\n","    [0.0,2.0,0.0],\n","    [0.0,3.0,0.0],\n","    [1.0,0.0,3.0],\n","    [1.0,1.0,0.0],\n","])"],"metadata":{"id":"az7A1JfAeN6p","executionInfo":{"status":"ok","timestamp":1690161608760,"user_tz":-540,"elapsed":6938,"user":{"displayName":"일땽","userId":"01657347254653910294"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["#### 입력준비\n","입력과 각 가중치를 내적"],"metadata":{"id":"fkxEGBJRd-7b"}},{"cell_type":"code","source":["# 쿼리, 키, 벨류 만들기\n","\n","keys = torch.matmul(x,w_key)\n","querys = torch.matmul(x,w_query)\n","values = torch.matmul(x,w_value)"],"metadata":{"id":"8srC4EJEd--C","executionInfo":{"status":"ok","timestamp":1690161613013,"user_tz":-540,"elapsed":2,"user":{"displayName":"일땽","userId":"01657347254653910294"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["### 스코어 계산\n","단어와 시퀀스 각 단어가 얼마나 관련이 있는가를 반영"],"metadata":{"id":"6qCTOxdnd_Ai"}},{"cell_type":"code","source":["querys"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2MWku5Fnd_Cq","executionInfo":{"status":"ok","timestamp":1690161744500,"user_tz":-540,"elapsed":10,"user":{"displayName":"일땽","userId":"01657347254653910294"}},"outputId":"5603f646-131a-4baf-9c6a-a406aae2ee3e"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 2.],\n","        [2., 2., 2.],\n","        [2., 1., 3.]])"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["keys.T"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lk9w4573d_E5","executionInfo":{"status":"ok","timestamp":1690161760292,"user_tz":-540,"elapsed":14,"user":{"displayName":"일땽","userId":"01657347254653910294"}},"outputId":"28250e3c-c499-4d96-cd5d-fbc74ada1cb0"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 4., 2.],\n","        [1., 4., 3.],\n","        [1., 0., 1.]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# 쿼이 행렬(Q)와 키 행렬(K)의 내적을 계산할 때 K를 전치하여 Q의 각 행(각 쿼리)과 K의 각 행(각 키) 간의 내적을 계산\n","# attn_scores 행렬의 각 원소는 쿼리의 각 단어와 키의 각 단어 간의 유사성(또는 관련성)을 나타낸다.\n","attn_scores = torch.matmul(querys, keys.T)"],"metadata":{"id":"lndycOw4d_Hi","executionInfo":{"status":"ok","timestamp":1690161859857,"user_tz":-540,"elapsed":6,"user":{"displayName":"일땽","userId":"01657347254653910294"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["attn_scores"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IKRuAswAd_Ja","executionInfo":{"status":"ok","timestamp":1690161928100,"user_tz":-540,"elapsed":434,"user":{"displayName":"일땽","userId":"01657347254653910294"}},"outputId":"d8b0fc5b-d515-4766-b9f9-8cd84e6588fa"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 2.,  4.,  4.],\n","        [ 4., 16., 12.],\n","        [ 4., 12., 10.]])"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["### 스코어 정규화\n","특정 단어가 각 단어에 얼마나 가중치를 둘 것인가를 계산. 모든 키에 대한 스코어 합이 1이 되도록 함"],"metadata":{"id":"0OSz_YqAd_Ly"}},{"cell_type":"code","source":["# 키 벡터의 차원수의 제곱근\n","import numpy as np\n","print(keys.shape)\n","np.sqrt(keys.shape[-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HTbUD-XAd_OD","executionInfo":{"status":"ok","timestamp":1690162007197,"user_tz":-540,"elapsed":7,"user":{"displayName":"일땽","userId":"01657347254653910294"}},"outputId":"c187a73b-df06-4971-f374-3218d3ab408a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 3])\n"]},{"output_type":"execute_result","data":{"text/plain":["1.7320508075688772"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# 소프트맥스 확률값 만들기\n","# 키 벡터의 차원수의 제곱근을 스케일링 요소로 사용. 키의 차원이 클 떄 attn_scores이 너무 커지는 것을 방기\n","# attn_scores 텐서의 마지막 차원(즉, 각 키에 대한 스코어)에 대해 softmax함수를 적용.\n","# 각 쿼리에 대해, 모든 키에 대한 스코어들이 합쳐져 1이 되도록 함.\n","\n","import numpy as np\n","from torch.nn.functional import softmax\n","\n","key_dim_sqrt = np.sqrt(keys.shape[-1])\n","attn_scores_softmax = softmax(attn_scores/ key_dim_sqrt, dim=-1) # dim= -1 은 softmax를 마지막 차원에 대해 적용"],"metadata":{"id":"Za216Og6gTA_","executionInfo":{"status":"ok","timestamp":1690162206785,"user_tz":-540,"elapsed":8,"user":{"displayName":"일땽","userId":"01657347254653910294"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["attn_scores_softmax"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jcjt9btCd_Qm","executionInfo":{"status":"ok","timestamp":1690162217566,"user_tz":-540,"elapsed":11,"user":{"displayName":"일땽","userId":"01657347254653910294"}},"outputId":"cec20749-e439-4c9f-9aca-5f08fe2fe8f2"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.3613e-01, 4.3194e-01, 4.3194e-01],\n","        [8.9045e-04, 9.0884e-01, 9.0267e-02],\n","        [7.4449e-03, 7.5471e-01, 2.3785e-01]])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# 특정 쿼리에 대해 sum을 계산\n","\n","attn_scores_softmax[2].sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ugYYVVcQd_TC","executionInfo":{"status":"ok","timestamp":1690162349853,"user_tz":-540,"elapsed":8,"user":{"displayName":"일땽","userId":"01657347254653910294"}},"outputId":"e7f9dd1a-bfcd-45fa-e19d-37b9eb5ade21"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.)"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["### 가중합 계산\n","정규화된 스코어를 각 값 벡터에 곱한 후, 모두 합하여 새로운 벡터를 생성하여 쿼리 단어의 문맥을 반영한 표현을 생성"],"metadata":{"id":"-14fHGyTd_WE"}},{"cell_type":"code","source":["# 소프트맥스 확률과 벨류를 가중합하기\n","# 정규화된 어텐션 스코어를 값 행렬에 곱하여 가중합을 계산\n","# 이 표현은 각 키와 연관된값의 가중합으로 구성. 이렇게 생성된 새로운 표현은 문장의 다은 부분에서 가져온 정보를 포함하므로.\n","# 쿼리 단어의 문맥을 반영한 표현\n","\n","weighted_values = torch.matmul(attn_scores_softmax, values)"],"metadata":{"id":"ALWF2575iJY5","executionInfo":{"status":"ok","timestamp":1690162547880,"user_tz":-540,"elapsed":390,"user":{"displayName":"일땽","userId":"01657347254653910294"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["weighted_values"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dVCv835HiW8x","executionInfo":{"status":"ok","timestamp":1690162557643,"user_tz":-540,"elapsed":6,"user":{"displayName":"일땽","userId":"01657347254653910294"}},"outputId":"52cecefa-2b4c-4b8d-8c19-572b20aae0a9"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.8639, 6.3194, 1.7042],\n","        [1.9991, 7.8141, 0.2735],\n","        [1.9926, 7.4796, 0.7359]])"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["### 출력 생성\n","모든 입력원소에 대해 위의 과정을 반복하고, 그 결과를 합쳐서 셀프 어텐션 레이어의 최종 출력을 생성. 이렇게 생성된 출력은 다음 레이어로 전달"],"metadata":{"id":"xw2KmoQKixw4"}},{"cell_type":"code","source":["## 트랜스포머 블록\n","- 인코더와 디코거 블록의 구조는 디테일에서 차이가 있을 뿐 본질적으로"],"metadata":{"id":"DaSM7I-vi7BB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"amtdHg_Gjn0p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# x 입력 w1은 입력층-은닉층을 연결하는 가중치, b1은 입력층-은닉층을 연결하는 바이어스\n","# w2는 은닉층-출력층을 연결하는 가중치, b2는 은닉층-출력층을 연경하는 바이어스\n","import torch\n","x = torch.tensor([2,1])\n","w1 = torch.tensor([[3,2,-4],[2,-3,1]])\n","b1 = 1\n","w2 = torch.tensor([[-1,1],[1,2],[3,1]])\n","b2 = -1"],"metadata":{"id":"OQ9447Pajnyn","executionInfo":{"status":"ok","timestamp":1690163023382,"user_tz":-540,"elapsed":377,"user":{"displayName":"일땽","userId":"01657347254653910294"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# 입력 데이터 x를 받아 두 번의 선형 변환과 ReLU 활성화 함수를 통과시켜 최종 출력 y를 생성하는 피드포워드 신경망(FFNN)\n","h_preact = torch.matmul(x,w1)+b1\n","h = torch.nn.functional.relu(h_preact)\n","y = torch.matmul(h,w2) + b2"],"metadata":{"id":"gc6Z0WODjnwx","executionInfo":{"status":"ok","timestamp":1690163238111,"user_tz":-540,"elapsed":8,"user":{"displayName":"일땽","userId":"01657347254653910294"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["print(x,x.shape)\n","print(w1,w1.shape)\n","print(h,h.shape)\n","print(w2,w2.shape)\n","print(y,y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2FHQp68zlE-G","executionInfo":{"status":"ok","timestamp":1690163308130,"user_tz":-540,"elapsed":384,"user":{"displayName":"일땽","userId":"01657347254653910294"}},"outputId":"8c2dbb54-419f-432b-e949-8360f2919808"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([2, 1]) torch.Size([2])\n","tensor([[ 3,  2, -4],\n","        [ 2, -3,  1]]) torch.Size([2, 3])\n","tensor([9, 2, 0]) torch.Size([3])\n","tensor([[-1,  1],\n","        [ 1,  2],\n","        [ 3,  1]]) torch.Size([3, 2])\n","tensor([-8, 12]) torch.Size([2])\n"]}]},{"cell_type":"markdown","source":["### 드롭아웃\n","- 딥러닝 모델은 그 표현력이 아주 좋아서 학습 데이터 그 자체를 외워버릴 염려가 있으며 이를 과적합(overfitting)이라고 한다.\n","- 드롭아웃(dropout)은 이러한 과적합 현상을 방지하고자 뉴런의 일부를 확률적으로 0으로 대치하여 계산에서 제외하는 기법"],"metadata":{"id":"SWVP-_OBjnuY"}},{"cell_type":"code","source":["import torch\n","m = torch.nn.Dropout(p=0.2)\n","input = torch.randn(1,10) # 1x10크기의 텐서를 생성\n","output = m(input)"],"metadata":{"id":"GJLMUGmijnsQ","executionInfo":{"status":"ok","timestamp":1690164349134,"user_tz":-540,"elapsed":4,"user":{"displayName":"일땽","userId":"01657347254653910294"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["input"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2dt6IMKyjnpw","executionInfo":{"status":"ok","timestamp":1690164388538,"user_tz":-540,"elapsed":412,"user":{"displayName":"일땽","userId":"01657347254653910294"}},"outputId":"f1e6ab7b-3143-4ae1-fd21-79a473afbd75"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.1479,  0.3047,  0.9700, -0.2410,  0.7397,  0.9238,  1.3021,  0.2107,\n","         -0.5352, -0.9390]])"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yrg3eZkUjnnY","executionInfo":{"status":"ok","timestamp":1690164359636,"user_tz":-540,"elapsed":8,"user":{"displayName":"일땽","userId":"01657347254653910294"}},"outputId":"3fc4f1a8-53f2-456d-ea2f-c196b8f83206"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.0000,  0.3808,  1.2126, -0.0000,  0.9246,  1.1548,  1.6277,  0.2634,\n","         -0.6690, -1.1737]])"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["### 아담 옵티마이저\n","- 딥러닝 모델 학습은 모델 출력과 정답 사이의 오차(error)를 최소화하는 방향을 구하고 이 방향에 맞춰 모델 전체의 파라미터(parameter)들을 업데이트하는 과정\n","- 오차를 손실(loss), 오차를 최소화하는 방향을 그래디언트(gradient)라고 합니다. 오차를 최소화하는 과정을 최적화(optimization)라고 한다.\n","- 오차를 구하려면 현재 시점의 모델에 입력을 넣어봐서 처음부터 끝까지 계산해보고 정답과 비교해야 하는데 오차를 구하기 위해 모델 처음부터 끝까지 순서대로 계산해보는 과정을 순전파(forward propagation)라고 한다.\n","- 오차를 구했다면 오차를 최소화하는 최초의 그래디언트를 구할 수 있다. 이는 미분(devative)으로 구하며 이후 미분의 연쇄 법칙(chain rule)에 따라 모델 각 가중치별 그래디언트 역시 구할 수 있으며 순전파의 역순으로 순차적으로 수행되서 이를 역전파(backpropagation)라고 한다.\n","- 아담 옵티마이저의 핵심 동작 원리는 방향과 보폭을 적절하게 정해주는 것이며 방향을 정할 때는 현재 위치에서 가장 경사가 급한 쪽으로 내려가되, 여태까지 내려오던 관성(방향)을 일부 유지하도록 한다. 보폭의 경우 안가본 곳은 성큼 빠르게 걸어 훑고 많이 가본 곳은 갈수록 보폭을 줄여 세밀하게 탐색하는 방식으로 정한다."],"metadata":{"id":"VvFnlMeGjnkY"}},{"cell_type":"markdown","source":["#### bert의 토크나이저 방법\n","- WordPiece 토크나이징 방식은 원래 문장을 더 작은 단위로 나누는 방법. 이 방식은 처음에는 각 문자를 개별 토큰으로 간주하고, 빈도 기반의 방법을 사용하여 반복적으로 가장 빈도가 높은 바이그램(두 문자의 연속)을 하나의 토큰으로 합치며 토큰 집합의 크기가 사전에 정한 어휘 크기에 도달할 때까지 반복.\n","-  예를 들어, 'unhappiness'라는 단어는 'un', '##happy', '##ness'라는 세 개의 토큰으로 분리될 수 있으며 '##' 기호는 해당 토큰이 원래 단어의 첫 토큰이 아니라는 것을 나타낸다.\n","- 이렇게 하면 BERT는 훈련 데이터에 없는 단어에 대해서도 일반화를 할 수 있게 되며, 이는 다양한 언어와 도메인에 대해 효과적으로 작동."],"metadata":{"id":"53LKf5OCpk94"}},{"cell_type":"markdown","source":["bert의 학습방법\n","사전 학습(Pre-training): 이 단계에서는 큰 텍스트 코퍼스(예: Wikipedia)를 사용하여 모델을 학습. BERT의 사전 학습은 두 가지 비지도 학습 방법을 사용.\n","\n","Masked Language Model(MLM): 일부 단어를 마스킹하고, 마스킹된 단어를 예측하도록 모델을 학습. 이를 통해 모델은 문맥을 이해하고, 주변 단어를 기반으로 단어를 예측하는 능력을 키울 수 있다.\n","\n","Next Sentence Prediction(NSP): 두 문장이 주어졌을 때, 두 번째 문장이 첫 번째 문장 다음에 오는 문장인지를 예측하도록 모델을 학습. 이를 통해 모델은 문장 간의 관계를 이해하는 능력을 키울 수 있다.\n","\n","파인 튜닝(Fine-tuning): 사전 학습 후, BERT 모델은 특정 작업에 대해 파인 튜닝 될 수 있다. 파인 튜닝은 레이블이 달린 작은 데이터셋을 사용하여 진행되며, 모든 레이어의 가중치를 업데이트. 이 단계에서는 특정 작업(예: 감성 분석, 질문 응답 등)의 학습 데이터를 사용하여 모델을 학습.\n","\n","이런 방식으로 BERT는 먼저 비지도 학습 방법을 통해 언어의 일반적인 패턴을 학습하고, 그 다음에는 지도 학습 방법을 통해 특정 작업에 맞게 모델을 조정. 이는 BERT가 다양한 자연어 처리 작업에 효과적으로 적용될 수 있게 만들어 준다."],"metadata":{"id":"Y0mOJ_Vfpk7f"}},{"cell_type":"markdown","source":["## GPT (Generative Pretrained Transformer)\n","- 트랜스포머 아키텍처의 디코더만 사용하는데, 그 이유는 GPT의 목표와 학습 방식에 기인.\n","- GPT는 주어진 문맥에 따라 텍스트를 생성하는 것을 목표로 하며 주어진 입력에 대해 가장 가능성 있는 다음 토큰(일반적으로 단어 또는 단어의 일부)을 예측하는 문제로 볼 수 있다.\n","- 입력 텍스트를 디코더에 직접 제공하고, 디코더는 각 단계에서 다음 토큰을 예측하게 되어 GPT는 주어진 문맥을 기반으로 텍스트를 생성하는 데 필요한 모델을 학습"],"metadata":{"id":"1Gx7Crizpk5H"}},{"cell_type":"markdown","source":["#### GPT 토큰화 방법\n","- GPT(Generative Pretrained Transformer)는 토큰화를 위해 Byte-Pair Encoding(BPE)라는 방법을 사용\n","- BPE는 원래의 텍스트에서 가장 빈번하게 등장하는 문자열 조합을 찾아서 하나의 새로운 '단어'를 만드는 과정을 반복적으로 수행하 자주 등장하는 단어는 하나의 토큰으로 묶이고, 그렇지 않은 단어는 여러 개의 토큰으로 분리\n","- 예를 들어 'lowest'라는 단어가 자주 등장하지 않고, 'low'라는 단어와 'est'라는 문자열이 자주 등장한다면, BPE는 'lowest'를 'low'와 'est' 두 개의 토큰으로 분리.\n","- 이런 방식으로 BPE는 텍스트를 더 작은 단위로 토큰화할 수 있으며, 이는 GPT와 같은 트랜스포머 기반 모델이 문맥을 보다 정확하게 이해하고, 더 큰 어휘를 처리"],"metadata":{"id":"gikfIa7dwwsv"}},{"cell_type":"markdown","source":["#### GPT 학습방법\n","- 사전 학습 (Pre-training): 이 단계에서 GPT는 대량의 텍스트 데이터를 학습하여 언어의 통계적 패턴을 이해. GPT는 이를 위해 'masked language model'이 아닌 'causal language model'을 사용. 즉, 주어진 문장에서 다음에 올 단어를 예측하는 방식으로 학습. 이 때문에 GPT는 문장을 앞에서부터 뒤로 읽으며 학습하게 되므로, 이를 'auto-regressive' 방식이라고 함.\n","\n","- 파인 튜닝 (Fine-tuning): 사전 학습 후, GPT는 특정 작업에 대해 파인 튜닝될 수 있다. 이 단계에서는 특정 작업(예: 감성 분석, 질문 응답 등)의 학습 데이터를 사용하여 모델을 학습시킵니다. 이때, 모델의 모든 가중치는 업데이트 가능하며, 파인 튜닝을 통해 특정 작업에 대해 최적화됩니다.\n","\n","- 따라서, GPT의 학습 방법은 사전 학습과 파인 튜닝 두 가지 주요 단계로 구성됩니다. 사전 학습 단계에서는 언어의 일반적인 패턴을 학습하고, 파인 튜닝 단계에서는 이를 바탕으로 특정 작업에 대해 최적화를 진행한다.\n","\n","- GPT의 원래 목적은 주어진 문맥에서 다음 단어를 예측하는 것이지만, 이런 방식은 기본적으로 언어의 구조와 문맥을 이해하는 능력을 요구. 따라서 이를 바탕으로 모델을 약간 수정하면, 문맥을 이해하고 특정 작업을 수행할 수 있는 모델을 만들 수 있다.\n","  - 예를 들어, 감성 분석 작업을 수행하기 위해 GPT를 파인 튜닝하는 경우, 모델의 출력 레이어를 각각의 감성 클래스에 대응하는 노드로 대체한 다음, 감성 레이블이 부여된 데이터셋을 사용하여 모델을 학습시키면, GPT는 입력 텍스트의 감성을 분류하는 데 필요한 패턴을 학습하게 된다."],"metadata":{"id":"__fefDvIwxhm"}},{"cell_type":"markdown","source":["- 마찬가지로, 질문 응답 시스템을 구축하기 위해 GPT를 파인 튜닝하는 경우, 모델의 입력을 질문과 문맥, 그리고 가능한 답변의 시작과 끝을 나타내는 토큰으로 구성한 다음, 질문과 그에 대한 정답이 포함된 데이터셋을 사용하여 모델을 학습시키면, GPT는 주어진 문맥에서 질문에 대한 적절한 답변을 생성하는 능력을 학습하게 된다.\n","\n","  따라서, GPT는 언어 생성 모델이지만, 파인 튜닝을 통해 다양한 자연어 처리 작업에 적용할 수 있다. 이는 GPT가 언어의 복잡한 패턴과 구조를 이해하는 능력을 가지고 있기 때문에 가능한 것이다."],"metadata":{"id":"OE1jp9pTwwpA"}},{"cell_type":"markdown","source":["## 파인튜닝\n","\n","- 문장을 워드피스(wordpiece)로 토큰화한 뒤 앞뒤에 문장 시작과 끝을 알리는 스페셜 토큰 CLS와 SEP를 각각 추가한 뒤 BERT에 입력\n","\n","- BERT 모델의 마지막 블록(레이어)의 출력 가운데 CLS에 해당하는 벡터를 추출. 트랜스포머 인코더 블록에서는 모든 단어가 서로 영향을 끼치기 때문에 마지막 블록 CLS 벡터는 문장 전체(이 영화 재미없네요)의 의미가 벡터 하나로 응집된 것이다.\n","\n","- 이렇게 뽑은 CLS 벡터에 작은 모듈을 하나 추가해, 그 출력이 미리 정해 놓은 범주(예컨대 긍정, 중립, 부정)가 될 확률이 되도록 한다. 학습 과정에서는 BERT와 그 위에 쌓은 작은 모듈을 포함한 전체 모델의 출력이 정답 레이블과 최대한 같아지도록 모델 전체를 업데이트. 이것이 파인튜닝(fine-tuning)\n","\n","- 문서 분류는 마지막 블록의 CLS 벡터만을 사용하는 반면, 개체명 인식 같은 과제에서는 마지막 블록의 모든 단어 벡터를 활용"],"metadata":{"id":"4vBRaDEEwwmv"}},{"cell_type":"code","source":[],"metadata":{"id":"D4IM_tWTwwk3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oQFJ9px3pkqP"},"execution_count":null,"outputs":[]}]}